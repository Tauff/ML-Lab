# -*- coding: utf-8 -*-
"""KNN_algo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pIZuyHOZTAH60xROp0Q01IboILDlWT68

# **KNN**
"""

# Importing required libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from matplotlib.colors import ListedColormap

# Load dataset and split features/target
data = pd.read_csv('suv_data.csv')
X, y = data.iloc[:, [2, 3]].values, data.iloc[:, -1].values

# Split into training and test sets, then scale features
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
scaler = StandardScaler()
X_train, X_test = scaler.fit_transform(X_train), scaler.transform(X_test)

# Train K-NN model and make predictions
knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

# Evaluate model performance
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Function to plot decision boundaries
def plot_boundary(X, y, title):
    X1, X2 = np.meshgrid(
        np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, 0.01),
        np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, 0.01)
    )
    plt.contourf(X1, X2, knn.predict(np.c_[X1.ravel(), X2.ravel()]).reshape(X1.shape), alpha=0.75, cmap=ListedColormap(['red', 'green']))
    for i, j in enumerate(np.unique(y)):
        plt.scatter(X[y == j, 0], X[y == j, 1], c=ListedColormap(['red', 'green'])(i), label=j)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

# Visualize decision boundaries for training and test sets
plot_boundary(X_train, y_train, "K-NN (Training Set)")
plot_boundary(X_test, y_test, "K-NN (Test Set)")

"""# **SVM**"""

# Importing required libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
from matplotlib.colors import ListedColormap

# Load dataset and split features/target
data = pd.read_csv('suv_data.csv')
X, y = data.iloc[:, [2, 3]].values, data.iloc[:, -1].values

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Scale features for better performance of SVM
scaler = StandardScaler()
X_train, X_test = scaler.fit_transform(X_train), scaler.transform(X_test)

# Train SVM model
svm = SVC(kernel='linear', random_state=0)  # You can use 'rbf', 'poly', etc., for different kernels
svm.fit(X_train, y_train)

# Predict test set results
y_pred = svm.predict(X_test)

# Evaluate model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Function to visualize decision boundary
def plot_boundary(X, y, title):
    X1, X2 = np.meshgrid(
        np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, 0.01),
        np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, 0.01)
    )
    plt.contourf(
        X1, X2, svm.predict(np.c_[X1.ravel(), X2.ravel()]).reshape(X1.shape),
        alpha=0.75, cmap=ListedColormap(['red', 'green'])
    )
    for i, j in enumerate(np.unique(y)):
        plt.scatter(X[y == j, 0], X[y == j, 1], c=ListedColormap(['red', 'green'])(i), label=j)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

# Visualize decision boundary for training and test sets
plot_boundary(X_train, y_train, "SVM (Training Set)")
plot_boundary(X_test, y_test, "SVM (Test Set)")

"""# Naive **Bayes**"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Sample documents
documents = ["I love programming", "Python is great", "I hate bugs", "Debugging is fun","Programming is also bad"]
labels = ["positive", "positive", "negative", "positive", "negative"]

# Using TF-IDF Vectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)

# Initialize and train the model
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

custom_text = ["ok","hate","good"]
custom_text = vectorizer.transform(custom_text)
prediction = model.predict(custom_text)
print("Prediction for custom text:", prediction)

"""# **Clustering Kmeans**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('Mall_Customers.csv')
data.head()

x = data.iloc[:,[3,4]].values

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters = 4, init = "k-means++", random_state = 42)
y_pred = kmeans.fit_predict(x)

plt.scatter(x[y_pred == 0, 0],x[y_pred == 0, 1], s=100, c="blue", label="Cluster 1")
plt.scatter(x[y_pred==1,0],x[y_pred==1,1], s=100, c="red", label="Cluster 2")
plt.scatter(x[y_pred==2,0],x[y_pred==2,1], s=100, c="pink", label="Cluster 3")
plt.scatter(x[y_pred==3,0],x[y_pred==3,1], s=100, c="grey", label="Cluster 4")
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, c="yellow", label="Centroid") #cluster
plt.title("K-Means Clustering")
plt.xlabel("3")
plt.ylabel("4")
plt.legend()
plt.show()

"""# **HMM**"""

from hmmlearn import hmm
import numpy as np

# Define the model parameters
states = ["Rainy", "Sunny"]
observations = ["Walk", "Shop", "Clean"]
n_states = len(states)
n_observations = len(observations)

# Transition probabilities
start_probability = np.array([0.6, 0.4])  # Initial state probabilities
transition_probability = np.array([
    [0.7, 0.3],  # Rainy -> (Rainy, Sunny)
    [0.4, 0.6],  # Sunny -> (Rainy, Sunny)
])

# Emission probabilities
emission_probability = np.array([
    [0.1, 0.4, 0.5],  # Rainy -> (Walk, Shop, Clean)
    [0.6, 0.3, 0.1],  # Sunny -> (Walk, Shop, Clean)
])

# Create a GaussianHMM (can use other models like MultinomialHMM if discrete)
model = hmm.MultinomialHMM(n_components=n_states)
model.startprob_ = start_probability
model.transmat_ = transition_probability
model.emissionprob_ = emission_probability

# Observations (encoded as integers)
observation_sequence = np.array([[0], [2], [1]])  # Walk, Clean, Shop

# Decode the sequence using the Viterbi algorithm
logprob, state_sequence = model.decode(observation_sequence, algorithm="viterbi")
print("Most likely states:", [states[state] for state in state_sequence])